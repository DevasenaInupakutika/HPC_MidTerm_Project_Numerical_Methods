{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preconditioned Conjugate Gradient (PCG) Solver Implementation (Symmetric Matrices)\n",
    "\n",
    "#Import statements for different libraries in Python\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Results directory\n",
    "res_dir = \"../Results/\"\n",
    "\n",
    "#Generating column vector row by 1 i.e. b\n",
    "def colN_by_1(row):\n",
    "    #Column Vector of row elements i.e. b\n",
    "    col_N = np.random.random_integers(1,10,size=(row,1))\n",
    "    return col_N\n",
    "\n",
    "#Results Data file Utility (create if not present else append the existing file with all result data)\n",
    "def res_data_file(N,str_result):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(res_dir):\n",
    "        os.makedirs(res_dir)\n",
    "        \n",
    "    # Create an file and write to it in append mode.\n",
    "    # Open a file with writing mode, and then just close it.\n",
    "    with open(os.path.join(res_dir, 'PCG_LSE_'+str(N)+'_by_'+str(N)), 'a') as resfile:\n",
    "        resfile.write(str_result)\n",
    "        resfile.close()\n",
    "\n",
    "#Preconditioned Conjugate Gradient Descent Function\n",
    "def pcg(A,b,x0,TOLERANCE, MAX_ITERATIONS,M):\n",
    "    \"\"\"\n",
    "    A function to solve [A]{x} = {b} linear equation system with the \n",
    "    preconditioned conjugate gradient method.\n",
    "    More at: https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_preconditioned_conjugate_gradient_method\n",
    "    ========== Parameters ==========\n",
    "    A : array \n",
    "        A real symmetric positive definite matrix.\n",
    "    b : vector\n",
    "        The right hand side (RHS) vector of the system.\n",
    "    x0 : vector\n",
    "        The starting guess for the solution.\n",
    "    MAX_ITERATIONS : integer\n",
    "        Maximum number of iterations. Iteration will stop after maxiter \n",
    "        steps even if the specified tolerance has not been achieved.\n",
    "    TOLERANCE : float\n",
    "        Tolerance to achieve. The algorithm will terminate when either \n",
    "        the relative or the absolute residual is below TOLERANCE.\n",
    "    M: {sparse matrix, dense matrix, LinearOperator}\n",
    "       Preconditioner for A. The preconditioner should approximate the inverse of A. \n",
    "       Effective preconditioning dramatically improves the rate of convergence, \n",
    "       which implies that fewer iterations are needed to reach a given error tolerance.    \n",
    "    \"\"\"\n",
    "    #   Initializations  \n",
    "    M_inv = np.linalg.inv(M) #Inverse of the preconditioner \n",
    "    \n",
    "    #Initialise the solution vector to the guess values\n",
    "    x = x0 \n",
    "    i = 0\n",
    "    \n",
    "    #Residue b-Ax\n",
    "    r = b - np.dot(A, x)\n",
    "    \n",
    "    #Preconditioning (Matrix M) is implemented in this solver for conjugate gradient (CG) to ensure convergence of CG\n",
    "    #Standard method is to multiply M_inv with the Ax = b equation. It is chosen such that it is better conditioned than \n",
    "    # A\n",
    "    #The preconditioner should approximate the inverse of A. Effective preconditioning dramatically improves the \n",
    "    #rate of convergence, which implies that fewer iterations are needed to reach a given error tolerance.\n",
    "    z = np.matmul(M_inv,r)\n",
    "    p = z\n",
    "    r_k_norm = np.dot(r.T,p)\n",
    "   \n",
    "    #Start iterations   \n",
    "    while i < MAX_ITERATIONS:\n",
    "        Ap = np.dot(A,p)\n",
    "        alpha = r_k_norm / np.dot(p.T,Ap)\n",
    "        x += alpha * p\n",
    "        r += alpha * Ap\n",
    "        r_kplus1_norm = np.dot(r.T,p)\n",
    "        beta = r_kplus1_norm / r_k_norm\n",
    "        r_k_norm = r_kplus1_norm\n",
    "        \n",
    "        #The algorithm terminates when either the relative or the absolute residual is below TOLERANCE\n",
    "        if r_kplus1_norm < TOLERANCE:\n",
    "            print 'Itr: ',i\n",
    "            break\n",
    "        p = p + beta * p \n",
    "        i+=1\n",
    "    return x\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #First command line parameter is dimension of a symmetric matrix and second command line parameter is maximum iterations\n",
    "    #Generating symmetric A matrix\n",
    "    N = int(sys.argv[1])\n",
    "    \n",
    "    #Maximum number of iterations to be entered as second command line argument\n",
    "    MAX_ITERATIONS = int(sys.argv[2])\n",
    "    #String representing number of iterations\n",
    "    str_iter = \"Maximum number of iterations: \"+str(MAX_ITERATIONS)\n",
    "    \n",
    "    #RHS Vector\n",
    "    #Generating a vector b depending on size N entered by user i.e. INITIALISE THE RHS VECTOR\n",
    "    b = colN_by_1(N)\n",
    "    \n",
    "    #Creating a symmetric matrix of size N by N\n",
    "    A = np.random.random_integers(1,10,size=(N,N))\n",
    "    A_symm = (A + A.T)/2\n",
    "    print(\"Symmetric matrix A is: \\n\",A_symm)\n",
    "    print(\"Vector b is: \\n\",b)\n",
    "    \n",
    "    \n",
    "    #Calling preconditioned conjugate gradient function\n",
    "    \n",
    "    #Tolerance to achieve. The algorithm will terminate when either the relative or the absolute residual is below TOLERANCE\n",
    "    TOLERANCE = 1.0e-05\n",
    "    \n",
    "    #Starting guess for the solution x0\n",
    "    x0 = np.zeros(shape=(N,1))\n",
    "    print(\"Initial guess: \\n\",x0)\n",
    "    \n",
    "    #Start the timer: for measuring the time of execution and getting the solution vector\n",
    "    start = time.clock()\n",
    "\n",
    "    #Solution vector with preconditioner M as approximation of inverse of A matrix\n",
    "    x_final = pcg(A_symm,b,x0,TOLERANCE,MAX_ITERATIONS,A_symm)\n",
    "    str_x = \"Solution Vector is: \", x_final\n",
    "    print(str_x)\n",
    "    \n",
    "    #Below steps are for measuring the error by computing L2 norm (residual norm error)\n",
    "    error_sum = 0\n",
    "    Ax = np.matmul(A_symm,x_final)\n",
    "    print(\"Ax is: \",Ax)\n",
    "\n",
    "    #Computing Solution error by calculating the L2 norm\n",
    "    for i in range(len(x_final)):\n",
    "        error_sum = error_sum + np.power((b[i] - Ax[i]),2)\n",
    "    \n",
    "    str_error_sum = \"Residual Norm error is: \", np.sqrt(error_sum) \n",
    "    print(str_error_sum)\n",
    "\n",
    "    #Measures time until this step and prints it\n",
    "    finish = time.clock()\n",
    "    runtime = finish - start\n",
    "\n",
    "    str_res_runtime = 'Time for solution is ', runtime,'s'\n",
    "    print(str_res_runtime)\n",
    "    \n",
    "    #Combined result data\n",
    "    str_result = str(str_x)+'\\n'+str(str_error_sum)+'\\n'+str(str_res_runtime)+'\\n'+str_iter\n",
    "\n",
    "    #Writing the size of linear system of equations (M by N), Solution vector, Error, Run time and Number of iterations to the results file\n",
    "    res_data_file(N,str_result)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
